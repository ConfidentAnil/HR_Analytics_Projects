{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fa3b028",
   "metadata": {},
   "source": [
    "# Building Linear Regression Model to Predict Premium Charges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c15bc12",
   "metadata": {},
   "source": [
    "* Importing Libraries\n",
    "* Import the dataset\n",
    "* Exploratory Data Analysis\n",
    "* Data Visualization\n",
    "* Feature Engineering/Selection\n",
    "* Model Building\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462ed57f",
   "metadata": {},
   "source": [
    "Importing libraries \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c492d555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6820473",
   "metadata": {},
   "source": [
    "Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b211d142",
   "metadata": {},
   "outputs": [],
   "source": [
    "insurance = pd.read_csv('insurance.csv')\n",
    "insurance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19792fc",
   "metadata": {},
   "source": [
    "EDA - Exploratory Data Analysis, we can find the following information about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ad31b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. There are 1138 records and 13 columns, 10 are of float datatype, and 3 are of object datatype\n",
    "2. past_consultations\tnum_of_steps\tHospital_expenditure\tNUmber_of_past_hospitalizations\tAnual_Salary\t are having\n",
    "high correlation in lowest to highest order.\n",
    "3. There are 52 null values in entire dataset\n",
    "\n",
    "'''\n",
    "insurance.head(10)\n",
    "insurance.shape\n",
    "insurance.dtypes\n",
    "insurance.info()\n",
    "insurance.isnull().sum(), insurance.isna().sum()\n",
    "insurance.corr(numeric_only = True)\n",
    "\n",
    "\n",
    "\n",
    "#Finding the percentage of null values \n",
    "'''\n",
    "Since the null values present in dataset for each column is less than 1% for almost all columns, we will drop the null values\n",
    "because it will not impact or effect the dataset.\n",
    "We drop null values : 1. When dataset is huge | 2. When there are little null values\n",
    "We will fill the null values : 1. When the dataset is small | 2. When there are more null values\n",
    "'''\n",
    "print(insurance.isnull().sum()/len(insurance)*100)\n",
    "insurance.dropna(inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Finding outlier of Claim_Amount column. We can find outliers for others columns also individually manually, we can automate the same using for loop\n",
    "q3 = insurance['Claim_Amount'].quantile(0.75)\n",
    "q1 = insurance['Claim_Amount'].quantile(0.25)\n",
    "UF = q3 + 1.5*IQR\n",
    "LF = q1 - 1.5*IQR\n",
    "insurance[insurance['Claim_Amount']>UF] #Values above Upper fence are outliers \n",
    "insurance[insurance['Claim_Amount']<LF] #Values below Lower fence are outliers\n",
    "insurance = insurance[(insurance['Claim_Amount']<=UF) & (insurance['Claim_Amount']>=LF)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626570a1",
   "metadata": {},
   "source": [
    "Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783107cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting countplot for 5 columns.\n",
    "countplot_columns = ['children', 'smoker', 'region', 'sex','NUmber_of_past_hospitalizations']\n",
    "\n",
    "plt.subplots(2,3, figsize=(15,5))\n",
    "plt.subplot(2,3,1)\n",
    "sns.countplot(x=insurance['children'])\n",
    "                \n",
    "plt.subplot(2,3,2)\n",
    "sns.countplot(x=insurance['smoker'])\n",
    "\n",
    "plt.subplot(2,3,3)\n",
    "sns.countplot(x=insurance['region'])\n",
    "\n",
    "plt.subplot(2,3,4)\n",
    "sns.countplot(x=insurance['sex'])\n",
    "\n",
    "plt.subplot(2,3,5)\n",
    "sns.countplot(x=insurance['NUmber_of_past_hospitalizations'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Seeing distribution of age. This shows the people from age group 20 to 23 are more\n",
    "sns.displot(insurance['age'])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Seeing the distribution of BMI. This shows the normal distribution it means most of the BMIs are concentrated at center\n",
    "#or at mean i.e. at 30 BMI and distributed equally on both the side, the frequency is decreasing from mean.\n",
    "sns.displot(insurance['bmi'])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624b5fc4",
   "metadata": {},
   "source": [
    "Feature Selection/Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1b9045",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label Encoding - It means converting the object type to numeric type data for making them a eligible feature for our training. \n",
    "#After label_encoding now we can plot a graph.\n",
    "#We can perform more operations on numeric type data. We convert the categorical data into labels, say flowers columns, with rose, lilly and sunflower, we labell rose = 1, lilly = 2, sunflower = 3\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#le object is inferring from LabelEncoder class \n",
    "le=LabelEncoder()\n",
    "#Transforming the categorical column into labels i.e. into numeric type data\n",
    "insurance['smoker'] = le.fit_transform(insurance['smoker'])\n",
    "insurance['region'] = le.fit_transform(insurance['region'])\n",
    "insurance['sex'] = le.fit_transform(insurance['sex'])\n",
    "\n",
    "\n",
    "\n",
    "#We are not dropping any features from the dataset we have just transformed or label encoded few columns.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe594b3f",
   "metadata": {},
   "source": [
    "Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150acf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the dependent and independent data\n",
    "x = insurance.drop('charges', axis = 1)\n",
    "y = insurance.iloc[:,-1] #Alternative ways : insurance.charges or insurance['charges']\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "#Splitting data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test =  train_test_split(x, y, train_size = 0.8, random_state = 0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Building linear regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "#Training the model\n",
    "lr.fit(x_train, y_train)\n",
    "y_pred = lr.predict(x_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Checking performance/accuracy of the model using Performance Metrics. * because it will pull out all the functions of classes from sklearn.metrics\n",
    "from sklearn.metrics import *\n",
    "r2_score(y_test, y_pred)\n",
    "#The R2 score is 0.99228 and it is a good accuracy of the model. To validate if the R2_score is good, we will plot a best fit line\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Creating Dataframe with Actual and Predicted values\n",
    "best_fit_line = pd.DataFrame(columns = ['Actual', 'Predicted'])\n",
    "best_fit_line['Actual'] = y_test\n",
    "best_fit_line['Predicted'] = y_pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Error or Residual between Actual and Predicted values\n",
    "best_fit_line['Error'] = best_fit_line['Actual'] - best_fit_line['Predicted']\n",
    "print(f'The actual and predicted values along with error/residual:\\n{best_fit_line}')\n",
    "\n",
    "\n",
    "\n",
    "#Plotting scatter plot to understand the actual vs predicted and drawing a best fit line\n",
    "plt.figure(figsize=(10,5))\n",
    "slope, intercept = np.polyfit(best_fit_line['Actual'], best_fit_line['Predicted'],1)\n",
    "best_fit_line1 = slope*best_fit_line['Actual'] + intercept\n",
    "sns.scatterplot(x=best_fit_line['Actual'], y=best_fit_line['Predicted'])\n",
    "plt.plot(best_fit_line['Actual'], best_fit_line1, color = 'black')\n",
    "plt.xlabel('True_Values')\n",
    "plt.ylabel('Predicted_Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b908f179",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dd2493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b1189b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff2a841",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
